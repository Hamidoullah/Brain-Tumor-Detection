{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ca1c6a-0d18-40d1-9232-7c2bd7b56775",
   "metadata": {},
   "source": [
    "Installer des versions spécifiques de PyTorch (2.1.0) et TorchVision (0.12.0) pour garantir leur compatibilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ef8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.1.0\n",
    "!pip torchvision==0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1280e59-4076-4684-8652-adfc9046840e",
   "metadata": {},
   "source": [
    "Afficher la version de TorchVision installée en important la bibliothèque et en imprimant sa version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b16098-e104-480b-bbac-349d89b54938",
   "metadata": {},
   "source": [
    "Importer les bibliothèques nécessaires pour manipuler les fichiers, effectuer des calculs numériques, créer et entraîner des réseaux de neurones, gérer des ensembles de données, transformer des images, et afficher une barre de progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f558e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0f691-c015-4855-a1e4-7a3a8b753948",
   "metadata": {},
   "source": [
    "La fonction load_images permet Charger des images à partir d'un dossier donné, les redimensionne, les normalise entre -1 et 1, et les convertit en tenseurs PyTorch au format (C, H, W) avant de les retourner sous forme de liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465082e0-50b1-4a45-a914-20f26affe9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Fonction de chargement des images -----------\n",
    "def load_images(data_path, img_shape):\n",
    "    images = []\n",
    "    for img_name in os.listdir(data_path):\n",
    "        img_path = os.path.join(data_path, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize(img_shape)\n",
    "        img = np.array(img) / 127.5 - 1  # Normalisation entre -1 et 1\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float()  # Changer l'ordre pour (C, H, W)\n",
    "        images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e4502-d448-4aee-8311-d45c7c81cd4c",
   "metadata": {},
   "source": [
    "Créer un dataset personnalisé pour charger et gérer des images, où chaque image est prétraitée (redimensionnée et normalisée) via la fonction load_images, avec un accès facile à leur longueur totale et à une image spécifique par son index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622fd1cf-9d94-415d-84be-2e68c4bd98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Dataset personnalisé -----------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_path, img_shape):\n",
    "        self.images = load_images(data_path, img_shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f0994-e6e1-4f80-97c1-dd07031ad9f7",
   "metadata": {},
   "source": [
    "Définir un générateur de réseau de neurones pour créer des images à partir d'un vecteur latent, en utilisant des couches entièrement connectées et transposées convolutives, suivi de normalisations et d'activations pour produire une image avec des valeurs normalisées entre -1 et 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5a4a20-d18d-446c-ae3b-804c36c2fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Générateur -----------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 32 * 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (128, 32, 32)),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.4),# 0.2\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff85ee3-d952-4391-83ad-581ac7910729",
   "metadata": {},
   "source": [
    "Définir un discriminateur, un réseau de neurones convolutif conçu pour différencier les images réelles des images générées, en utilisant des couches convolutives, des normalisations, des activations et une sortie sigmoïde pour produire une probabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f9088b-122e-4821-983a-d3d3911f8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Discriminateur -----------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.4), #0.2\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 32 * 32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611b501-f275-4ef2-a477-8d522cfacd69",
   "metadata": {},
   "source": [
    "Initialiser les paramètres du modèle et les hyperparamètres : dimension du vecteur latent (100), taille des images (128x128), chemin des données, taille de batch (64), nombre d'époques (2000), taux d'apprentissage (0.0002), et sélection de l'appareil (GPU si disponible, sinon CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e766ef0e-791e-4b88-bf47-ef73e9941c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Initialisation et hyperparamètres -----------\n",
    "latent_dim = 100\n",
    "img_shape = (128, 128)\n",
    "data_path = \"Dataset/tumor\"\n",
    "batch_size = 64\n",
    "num_epochs = 2000\n",
    "lr = 0.0002\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fffcf1-ecb4-4d0c-a6fc-62464ad0c9e9",
   "metadata": {},
   "source": [
    "Charger les données avec un DataLoader, crée les modèles de générateur et de discriminateur, initialise les optimiseurs avec l'algorithme Adam et définit la fonction de perte (BCELoss) pour l'entraînement du GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b403b8dd-03d9-4dde-8e04-3625aa7e446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "dataset = CustomImageDataset(data_path, img_shape)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Créer les modèles\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimiseurs\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Fonction de perte\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf2bfd-fc6f-41e4-b2fc-ef5562d98bd0",
   "metadata": {},
   "source": [
    "Entraîner un GAN pendant 2000 époques en optimisant le générateur et le discriminateur. À chaque époque, le générateur crée de fausses images et le discriminateur apprend à distinguer les vraies des fausses. Les pertes des deux modèles sont calculées et les paramètres sont mis à jour. Toutes les 100 époques, des images générées sont sauvegardées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e8ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296ff7a513c3463d913005757f762355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training GAN:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000 | D Loss: 0.0030 | G Loss: 5.3677\n",
      "Epoch 2/2000 | D Loss: 0.0043 | G Loss: 5.3135\n",
      "Epoch 3/2000 | D Loss: 0.0026 | G Loss: 5.6288\n",
      "Epoch 4/2000 | D Loss: 0.0016 | G Loss: 5.9461\n",
      "Epoch 5/2000 | D Loss: 0.0018 | G Loss: 6.0075\n",
      "Epoch 6/2000 | D Loss: 0.0032 | G Loss: 5.8974\n",
      "Epoch 7/2000 | D Loss: 0.0011 | G Loss: 6.6582\n",
      "Epoch 8/2000 | D Loss: 0.0041 | G Loss: 6.4577\n",
      "Epoch 9/2000 | D Loss: 0.0053 | G Loss: 5.5889\n",
      "Epoch 10/2000 | D Loss: 1.4655 | G Loss: 9.6110\n",
      "Epoch 11/2000 | D Loss: 0.0893 | G Loss: 8.1159\n",
      "Epoch 12/2000 | D Loss: 2.4654 | G Loss: 25.9388\n",
      "Epoch 13/2000 | D Loss: 0.8936 | G Loss: 6.4326\n",
      "Epoch 14/2000 | D Loss: 0.7343 | G Loss: 16.1921\n",
      "Epoch 15/2000 | D Loss: 0.0000 | G Loss: 14.9037\n",
      "Epoch 16/2000 | D Loss: 0.3706 | G Loss: 20.4235\n",
      "Epoch 17/2000 | D Loss: 0.0105 | G Loss: 7.5242\n",
      "Epoch 18/2000 | D Loss: 0.8603 | G Loss: 6.5126\n",
      "Epoch 19/2000 | D Loss: 0.0576 | G Loss: 4.8867\n",
      "Epoch 20/2000 | D Loss: 0.0270 | G Loss: 4.1122\n",
      "Epoch 21/2000 | D Loss: 0.0515 | G Loss: 5.0963\n",
      "Epoch 22/2000 | D Loss: 0.2441 | G Loss: 11.8369\n",
      "Epoch 23/2000 | D Loss: 0.0145 | G Loss: 4.8711\n",
      "Epoch 24/2000 | D Loss: 0.1280 | G Loss: 3.5540\n",
      "Epoch 25/2000 | D Loss: 0.1129 | G Loss: 2.9817\n",
      "Epoch 26/2000 | D Loss: 0.1630 | G Loss: 2.9998\n",
      "Epoch 27/2000 | D Loss: 0.0548 | G Loss: 5.4743\n",
      "Epoch 28/2000 | D Loss: 0.0614 | G Loss: 3.1612\n",
      "Epoch 29/2000 | D Loss: 0.0527 | G Loss: 4.5770\n",
      "Epoch 30/2000 | D Loss: 0.0153 | G Loss: 6.2683\n",
      "Epoch 31/2000 | D Loss: 0.5499 | G Loss: 2.2966\n",
      "Epoch 32/2000 | D Loss: 0.0283 | G Loss: 7.8097\n",
      "Epoch 33/2000 | D Loss: 0.0131 | G Loss: 6.5437\n",
      "Epoch 34/2000 | D Loss: 0.0876 | G Loss: 5.3675\n",
      "Epoch 35/2000 | D Loss: 0.0703 | G Loss: 4.8348\n",
      "Epoch 36/2000 | D Loss: 0.0242 | G Loss: 5.7414\n",
      "Epoch 37/2000 | D Loss: 0.0149 | G Loss: 5.3017\n",
      "Epoch 38/2000 | D Loss: 0.0188 | G Loss: 4.5514\n",
      "Epoch 39/2000 | D Loss: 0.0799 | G Loss: 5.5403\n",
      "Epoch 40/2000 | D Loss: 0.4127 | G Loss: 7.7783\n",
      "Epoch 41/2000 | D Loss: 0.0234 | G Loss: 10.1563\n",
      "Epoch 42/2000 | D Loss: 0.0279 | G Loss: 4.6599\n",
      "Epoch 43/2000 | D Loss: 0.0267 | G Loss: 4.4775\n",
      "Epoch 44/2000 | D Loss: 0.0325 | G Loss: 3.8866\n",
      "Epoch 45/2000 | D Loss: 0.0285 | G Loss: 3.8893\n",
      "Epoch 46/2000 | D Loss: 0.0217 | G Loss: 5.4903\n",
      "Epoch 47/2000 | D Loss: 0.1619 | G Loss: 4.9109\n",
      "Epoch 48/2000 | D Loss: 0.0081 | G Loss: 7.3866\n",
      "Epoch 49/2000 | D Loss: 0.0053 | G Loss: 6.7879\n",
      "Epoch 50/2000 | D Loss: 0.0502 | G Loss: 5.6149\n",
      "Epoch 51/2000 | D Loss: 0.1660 | G Loss: 5.4374\n",
      "Epoch 52/2000 | D Loss: 0.0128 | G Loss: 5.0825\n",
      "Epoch 53/2000 | D Loss: 0.0800 | G Loss: 6.5533\n",
      "Epoch 54/2000 | D Loss: 0.2084 | G Loss: 8.0725\n",
      "Epoch 55/2000 | D Loss: 0.0098 | G Loss: 8.5484\n",
      "Epoch 56/2000 | D Loss: 0.1009 | G Loss: 6.3839\n",
      "Epoch 57/2000 | D Loss: 0.0298 | G Loss: 4.5268\n",
      "Epoch 58/2000 | D Loss: 0.0478 | G Loss: 3.7677\n",
      "Epoch 59/2000 | D Loss: 0.0132 | G Loss: 5.2712\n",
      "Epoch 60/2000 | D Loss: 0.0294 | G Loss: 5.0784\n",
      "Epoch 61/2000 | D Loss: 0.0194 | G Loss: 5.3452\n",
      "Epoch 62/2000 | D Loss: 0.5615 | G Loss: 0.6109\n",
      "Epoch 63/2000 | D Loss: 0.0054 | G Loss: 7.9868\n",
      "Epoch 64/2000 | D Loss: 0.1632 | G Loss: 3.0616\n"
     ]
    }
   ],
   "source": [
    "# ----------- Entraînement -----------\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training GAN\"):\n",
    "# for epoch in range(num_epochs):\n",
    "    for i, real_images in enumerate(data_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # Vrais et faux labels\n",
    "        valid = torch.ones((batch_size, 1), device=device)\n",
    "        fake = torch.zeros((batch_size, 1), device=device)\n",
    "        \n",
    "        # -------- Entraînement du générateur --------\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        generated_images = generator(z)\n",
    "        g_loss = criterion(discriminator(generated_images), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # -------- Entraînement du discriminateur --------\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_images), valid)\n",
    "        fake_loss = criterion(discriminator(generated_images.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    # Sauvegarde des images toutes les 100 époques\n",
    "    if epoch % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            for idx, img in enumerate(generated_images):\n",
    "                save_image(img, f\"torch_generated_image_epoch_{epoch}_img_{idx}.png\", normalize=True)\n",
    "#         with torch.no_grad():\n",
    "#             generated_images = generator(z).detach().cpu()\n",
    "#             save_image(generated_images, f\"torch_generated_images_epoch_{epoch}.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5e171-8893-4f70-b31d-bddd077d4b12",
   "metadata": {},
   "source": [
    "Enfin, après l'entraînement, le générateur produit et sauvegarde 16 nouvelles images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646ff19-a108-416f-8265-78a2993be3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Générer des images après l'entraînement -----------\n",
    "z = torch.randn(16, latent_dim, device=device)\n",
    "generated_images = generator(z)\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(generated_images):\n",
    "        save_image(img, f\"torch_final_generated_image_{idx}.png\", normalize=True)\n",
    "# z = torch.randn(16, latent_dim, device=device)\n",
    "# generated_images = generator(z)\n",
    "# save_image(generated_images, \"torch_final_generated_images.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcb542-f8d4-44ed-bc48-3200f1f0c107",
   "metadata": {},
   "source": [
    "Sauvegarder les poids du modèle du générateur et du discriminateur dans des fichiers .pth afin de pouvoir les recharger et les réutiliser plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), f\"generator_model.pth\")\n",
    "torch.save(discriminator.state_dict(), f\"discriminator_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a5552-644e-4f79-9582-1c7bf3589190",
   "metadata": {},
   "source": [
    "Charger les modèles préalablement sauvegardés en réinitialisant les instances du générateur et du discriminateur, puis charge les poids des modèles sauvegardés. Les modèles sont ensuite mis en mode évaluation pour désactiver certaines opérations comme la normalisation par lot pendant l'inférence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "# Créer des instances des modèles\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Charger les états sauvegardés\n",
    "generator.load_state_dict(torch.load(\"generator_model.pth\"))\n",
    "discriminator.load_state_dict(torch.load(\"discriminator_model.pth\"))\n",
    "\n",
    "# Mettre en mode évaluation\n",
    "generator.eval()\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b45565-d8d4-4d7b-bedc-dcd6785d8b3d",
   "metadata": {},
   "source": [
    "Génèrer 408 images à partir de vecteurs aléatoires en utilisant le générateur, et les sauvegarde sous forme de fichiers .jpg dans le dossier \"FacticeImage\", avec un nom de fichier qui inclut l'index de l'image. Les images sont générées sans calcul de gradients pour économiser de la mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(408):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(1, latent_dim, device=device)  # Génère 1 vecteurs de bruit\n",
    "        generated_images = generator(z).detach().cpu()\n",
    "        save_image(generated_images, f\"FacticeImage/tumor ({i}).jpg\", normalize=True)\n",
    "# with torch.no_grad():\n",
    "#     z = torch.randn(16, latent_dim, device=device)  # Génère 16 vecteurs de bruit\n",
    "#     generated_images = generator(z).detach().cpu()\n",
    "#     save_image(generated_images, \"tumor.jpg\", normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
